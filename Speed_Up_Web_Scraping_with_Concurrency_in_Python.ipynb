{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5vypQX705N+Jr2MLDL04Z"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@datajournal/speed-up-web-scraping-with-concurrency-in-python-ce25839f9399)"
      ],
      "metadata": {
        "id": "n-hH_jHZYgKD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VOk4xSSXYePg"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "base_url = \"https://example.com/products/page\"\n",
        "pages = range(1, 13) # Scrape 12 pages\n",
        "\n",
        "def extract_data(page):\n",
        "    url = f\"{base_url}/{page}/\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Extract data here (e.g., product name, price)\n",
        "    products = []\n",
        "    for product in soup.select(\".product\"):\n",
        "        products.append({\n",
        "            \"name\": product.find(\"h2\").text.strip(),\n",
        "            \"price\": product.find(class_=\"price\").text.strip(),\n",
        "            \"url\": product.find(\"a\").get(\"href\")\n",
        "        })\n",
        "    return products\n",
        "\n",
        "def store_results(products):\n",
        "    with open(\"products.csv\", \"w\") as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=products[0].keys())\n",
        "        writer.writeheader()\n",
        "        writer.writerows(products)\n",
        "\n",
        "all_products = []\n",
        "\n",
        "for page in pages:\n",
        "    all_products.extend(extract_data(page))\n",
        "\n",
        "store_results(all_products)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aiohttp beautifulsoup4"
      ],
      "metadata": {
        "id": "kFgsQjTtY1Gn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import aiohttp\n",
        "import asyncio\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "base_url = \"https://example.com/products/page\"\n",
        "pages = range(1, 13) # Scrape 12 pages\n",
        "    async def extract_data(page, session):\n",
        "        url = f\"{base_url}/{page}/\"\n",
        "        async with session.get(url) as response:\n",
        "            soup = BeautifulSoup(await response.text(), \"html.parser\")\n",
        "            products = []\n",
        "            for product in soup.select(\".product\"):\n",
        "                products.append({\n",
        "                \"name\": product.find(\"h2\").text.strip(),\n",
        "                \"price\": product.find(class_=\"price\").text.strip(),\n",
        "                \"url\": product.find(\"a\").get(\"href\")\n",
        "                })\n",
        "        return products\n",
        "async def main():\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = [extract_data(page, session) for page in pages]\n",
        "        results = await asyncio.gather(*tasks)\n",
        "        all_products = [item for sublist in results for item in sublist] # Flatten the list\n",
        "        store_results(all_products)\n",
        "def store_results(products):\n",
        "    with open(\"products.csv\", \"w\") as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=products[0].keys())\n",
        "        writer.writeheader()\n",
        "        writer.writerows(products)\n",
        "# Run the asyncio event loop\n",
        "asyncio.run(main())"
      ],
      "metadata": {
        "id": "BUyNlwwWYyUX"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}
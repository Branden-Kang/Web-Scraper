{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3MfvYk22pWRxMhvFgLs5P"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@datajournal/speed-up-web-scraping-b045b9da0b1e)"
      ],
      "metadata": {
        "id": "9KzDUOHHIuYF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mlqxAdLPIr2w"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "base_url = \"https://example.com/products/page\"\n",
        "pages = range(1, 13)  # Scrape 12 pages\n",
        "\n",
        "def extract_data(page):\n",
        "    url = f\"{base_url}/{page}/\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Extract data here (e.g., product name, price)\n",
        "    products = []\n",
        "    for product in soup.select(\".product\"):\n",
        "        products.append({\n",
        "            \"name\": product.find(\"h2\").text.strip(),\n",
        "            \"price\": product.find(class_=\"price\").text.strip(),\n",
        "            \"url\": product.find(\"a\").get(\"href\")\n",
        "        })\n",
        "\n",
        "    return products\n",
        "\n",
        "def store_results(products):\n",
        "    with open(\"products.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=products[0].keys())\n",
        "        writer.writeheader()\n",
        "        writer.writerows(products)\n",
        "\n",
        "all_products = []\n",
        "for page in pages:\n",
        "    all_products.extend(extract_data(page))\n",
        "\n",
        "store_results(all_products)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install aiohttp beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASs5F-cWJkRB",
        "outputId": "218d0ae2-9b7b-4222-88e8-de47594e7c97"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (3.11.13)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (25.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.18.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import aiohttp\n",
        "import asyncio\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "base_url = \"https://example.com/products/page\"\n",
        "pages = range(1, 13)  # Scrape 12 pages\n",
        "\n",
        "async def extract_data(page, session):\n",
        "    url = f\"{base_url}/{page}/\"\n",
        "    async with session.get(url) as response:\n",
        "        soup = BeautifulSoup(await response.text(), \"html.parser\")\n",
        "\n",
        "        products = []\n",
        "        for product in soup.select(\".product\"):\n",
        "            products.append({\n",
        "                \"name\": product.find(\"h2\").text.strip(),\n",
        "                \"price\": product.find(class_=\"price\").text.strip(),\n",
        "                \"url\": product.find(\"a\").get(\"href\")\n",
        "            })\n",
        "\n",
        "        return products\n",
        "\n",
        "async def main():\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = [extract_data(page, session) for page in pages]\n",
        "        results = await asyncio.gather(*tasks)\n",
        "\n",
        "        # Flatten the list\n",
        "        all_products = [item for sublist in results for item in sublist]\n",
        "        store_results(all_products)\n",
        "\n",
        "def store_results(products):\n",
        "    with open(\"products.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=products[0].keys())\n",
        "        writer.writeheader()\n",
        "        writer.writerows(products)\n",
        "\n",
        "# Run the asyncio event loop\n",
        "asyncio.run(main())"
      ],
      "metadata": {
        "id": "IOKSRHDVJh_O"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from bs4 import BeautifulSoup\n",
        "import aiohttp\n",
        "\n",
        "max_concurrency = 5  # Limit concurrent requests\n",
        "sem = asyncio.Semaphore(max_concurrency)\n",
        "\n",
        "async def extract_data(page, session):\n",
        "    async with sem:  # This limits the number of concurrent requests\n",
        "        url = f\"{base_url}/{page}/\"\n",
        "        async with session.get(url) as response:\n",
        "            soup = BeautifulSoup(await response.text(), \"html.parser\")\n",
        "\n",
        "            products = []\n",
        "            for product in soup.select(\".product\"):\n",
        "                products.append({\n",
        "                    \"name\": product.find(\"h2\").text.strip(),\n",
        "                    \"price\": product.find(class_=\"price\").text.strip(),\n",
        "                    \"url\": product.find(\"a\").get(\"href\")\n",
        "                })\n",
        "\n",
        "            return products"
      ],
      "metadata": {
        "id": "OenrzsZrKh_r"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}
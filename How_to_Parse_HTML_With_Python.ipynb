{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMujnGwRDTCU3zELCJbS1Xw"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@datajournal/how-to-parse-html-with-python-94495c11bc96)"
      ],
      "metadata": {
        "id": "ezHCNbtWyRHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BeautifulSoup"
      ],
      "metadata": {
        "id": "XGdXjtVgyhhB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoDEUAylyOzt",
        "outputId": "4c07b8ac-dce3-46d0-ae27-5aee6f1a70c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page Title: Example Domain\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Fetch the HTML content of the webpage\n",
        "url = \"https://example.com\"\n",
        "response = requests.get(url)\n",
        "# Parse the HTML content using BeautifulSoup\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "# Extract the title of the webpage\n",
        "title = soup.title.text\n",
        "print(\"Page Title:\", title)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lxml"
      ],
      "metadata": {
        "id": "4aymLUISynnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lxml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yk3l8lrVyeo8",
        "outputId": "19db9267-579e-4843-cd12-df980144a8fc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from lxml import html\n",
        "import requests\n",
        "# Fetch the HTML content\n",
        "url = \"https://example.com\"\n",
        "response = requests.get(url)\n",
        "# Parse the HTML content using lxml\n",
        "tree = html.fromstring(response.content)\n",
        "# Extract the title of the webpage\n",
        "title = tree.findtext('.//title')\n",
        "print(\"Page Title:\", title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Cs3Vl8Yydxh",
        "outputId": "62ec0ce2-403b-4711-f467-3bb79bc219d2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page Title: Example Domain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract all links using XPath\n",
        "links = tree.xpath('//a/@href')\n",
        "for link in links:\n",
        "    print(link)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YL_9HRGlytCD",
        "outputId": "563eb5b1-6a97-433e-d2c8-f6e683f95d4e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.iana.org/domains/example\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# html.parser"
      ],
      "metadata": {
        "id": "H2mEdYckywY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from html.parser import HTMLParser\n",
        "\n",
        "class MyHTMLParser(HTMLParser):\n",
        "    def handle_starttag(self, tag, attrs):\n",
        "        print(\"Start tag:\", tag)\n",
        "    def handle_endtag(self, tag):\n",
        "        print(\"End tag:\", tag)\n",
        "    def handle_data(self, data):\n",
        "        print(\"Data:\", data)\n",
        "\n",
        "# Sample HTML to parse\n",
        "html_content = \"\"\"\n",
        "<html>\n",
        "<head><title>Example</title></head>\n",
        "<body><p>Hello, world!</p></body>\n",
        "</html>\n",
        "\"\"\"\n",
        "# Create an instance of the parser and feed it the HTML content\n",
        "parser = MyHTMLParser()\n",
        "parser.feed(html_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMe5joJYyvTZ",
        "outputId": "eb733455-ccee-4e45-df57-72490de022a2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data: \n",
            "\n",
            "Start tag: html\n",
            "Data: \n",
            "\n",
            "Start tag: head\n",
            "Start tag: title\n",
            "Data: Example\n",
            "End tag: title\n",
            "End tag: head\n",
            "Data: \n",
            "\n",
            "Start tag: body\n",
            "Start tag: p\n",
            "Data: Hello, world!\n",
            "End tag: p\n",
            "End tag: body\n",
            "Data: \n",
            "\n",
            "End tag: html\n",
            "Data: \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Selenium"
      ],
      "metadata": {
        "id": "9jUVfLXBzL0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euLSdqyIzJwR",
        "outputId": "8ac2b4bd-82b4-4af3-d02e-c3d581addcb3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.25.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.27.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.8.30)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.2.0)\n",
            "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Downloading selenium-4.25.0-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.27.0-py3-none-any.whl (481 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sortedcontainers, outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.25.0 sortedcontainers-2.4.0 trio-0.27.0 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "# Set up the Selenium driver (ensure you have a driver like ChromeDriver installed)\n",
        "driver = webdriver.Chrome()\n",
        "# Open the webpage\n",
        "url = \"https://example.com\"\n",
        "driver.get(url)\n",
        "# Get the page source after JavaScript has loaded the content\n",
        "html_content = driver.page_source\n",
        "# Parse the HTML using BeautifulSoup\n",
        "soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "# Extract the title\n",
        "title = soup.title.text\n",
        "print(\"Page Title:\", title)\n",
        "# Close the Selenium driver\n",
        "driver.quit()"
      ],
      "metadata": {
        "id": "RPE5JhPJyzVb"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}
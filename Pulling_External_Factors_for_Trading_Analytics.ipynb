{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pulling External Factors for Trading Analytics.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN8iGN6+BU8Dyrda4+bLr8h"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@oleg.kazanskyi/trading-analytics-with-financial-kpis-33df3b1cff24)"
      ],
      "metadata": {
        "id": "2KYK27nyOw1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6gKJGklO6IW",
        "outputId": "cbf462fc-535d-42dc-95d7-ec6bd4dc8afa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.4.0-py3-none-any.whl (985 kB)\n",
            "\u001b[K     |████████████████████████████████| 985 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting trio~=0.17\n",
            "  Downloading trio-0.21.0-py3-none-any.whl (358 kB)\n",
            "\u001b[K     |████████████████████████████████| 358 kB 61.2 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting urllib3[secure,socks]~=1.26\n",
            "  Downloading urllib3-1.26.11-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 44.9 MB/s \n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (22.1.0)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
            "Collecting cryptography>=1.3.4\n",
            "  Downloading cryptography-37.0.4-cp36-abi3-manylinux_2_24_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 53.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (2022.6.15)\n",
            "Collecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.1.1)\n",
            "Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.11 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-37.0.4 h11-0.13.0 outcome-1.2.0 pyOpenSSL-22.0.0 selenium-4.4.0 sniffio-1.2.0 trio-0.21.0 trio-websocket-0.9.2 urllib3-1.26.11 wsproto-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VVnH3TcpOveR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import datetime\n",
        "from datetime import date\n",
        "\n",
        "def get_vix(last_date = date.today(), historical_days = 1450):\n",
        "    historical_date = last_date-datetime.timedelta(days=historical_days)\n",
        "    \n",
        "    url=\"https://cdn.cboe.com/api/global/us_indices/daily_prices/VIX_History.csv\"\n",
        "    response=pd.read_csv(url)\n",
        "    \n",
        "    response[\"DATE\"]=pd.to_datetime(response[\"DATE\"])\n",
        "    response.rename(columns = {\"DATE\":\"date\"}, inplace = True)\n",
        "    response.set_index([\"date\"], inplace = True)\n",
        "    response = response[historical_date:last_date][\"HIGH\"]\n",
        "    response = response.rename(\"VIX_high\")\n",
        "    \n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "from datetime import date\n",
        "from selenium import webdriver\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def download_tables(last_date = date.today(), historical_days = 1450):\n",
        "    '''\n",
        "    Downloads 10-Y bond historical daily data\n",
        "    '''\n",
        "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
        "    driver.get('https://finance.yahoo.com/quote/%5ETNX/history?period1=1492732800&period2=1650326400&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true')\n",
        "    \n",
        "    #remove old file with data if it's available\n",
        "    filePath = 'C:\\\\Users\\\\oleg.kazanskyi\\\\Downloads\\\\^TNX.csv'\n",
        "            \n",
        "    # Check whether the  specified file is available and remove it  \n",
        "    if os.path.exists(filePath):\n",
        "        os.remove(filePath)\n",
        "        print(\"We remove old csv file with metadata and replace with the newer one\")\n",
        "    \n",
        "    #Calculating current and historical periods\n",
        "    todays_year = last_date.year\n",
        "    todays_month = last_date.month\n",
        "    todays_day = last_date.day\n",
        "    historical_date = last_date-datetime.timedelta(days=historical_days)\n",
        "    historical_year = historical_date.year\n",
        "    historical_month = historical_date.month\n",
        "    historical_day = historical_date.day\n",
        "    \n",
        "    #Searching an input field for dates\n",
        "    elem = driver.find_element_by_xpath('//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[1]/div[1]/div[1]/div/div/div/span')\n",
        "    elem.click()\n",
        "    time.sleep(3)\n",
        "    \n",
        "    #Enter historical date\n",
        "    elem = driver.find_element_by_xpath('//*[@id=\"dropdown-menu\"]/div/div[1]/input')\n",
        "    elem.send_keys(historical_year)\n",
        "    elem.send_keys(Keys.TAB)\n",
        "    elem.send_keys(historical_month)\n",
        "    elem.send_keys(historical_day)\n",
        "    time.sleep(3)\n",
        "    \n",
        "    #Enter last date\n",
        "    elem = driver.find_element_by_xpath('//*[@id=\"dropdown-menu\"]/div/div[2]/input')\n",
        "    elem.send_keys(todays_year)\n",
        "    elem.send_keys(Keys.TAB)\n",
        "    elem.send_keys(todays_month)\n",
        "    elem.send_keys(todays_day)\n",
        "    time.sleep(3)\n",
        "    \n",
        "    #Press done after entering dates\n",
        "    elem = driver.find_element_by_xpath('//*[@id=\"dropdown-menu\"]/div/div[3]/button[1]')\n",
        "    elem.click()\n",
        "    time.sleep(3)\n",
        "    \n",
        "    #Press apply dates range\n",
        "    elem = driver.find_element_by_xpath('//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[1]/div[1]/button')\n",
        "    elem.click()\n",
        "    time.sleep(3)\n",
        "    \n",
        "    #press download data\n",
        "    elem = driver.find_element_by_xpath('//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[1]/div[2]/span[2]/a/span')\n",
        "    elem.click()\n",
        "    time.sleep(3)\n",
        "    \n",
        "    driver.close\n",
        "    \n",
        "    data = pd.read_csv(\"C:\\\\Users\\\\oleg.kazanskyi\\\\Downloads\\\\^TNX.csv\")\n",
        "    \n",
        "    data.Date = pd.to_datetime(data.Date)\n",
        "    data.rename(columns = {\"Date\":\"date\", \"Adj Close\":\"10Y_bonds\"}, inplace = True)\n",
        "    data.set_index([\"date\"], inplace = True)\n",
        "    data = data[\"10Y_bonds\"]\n",
        "    data.dropna(axis = 0, inplace = True)\n",
        "    \n",
        "    return data"
      ],
      "metadata": {
        "id": "ALkDwAlgO2Pf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "from datetime import date\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def get_dates():\n",
        "    last_date = date.today()\n",
        "    historical_days = 1450\n",
        "    historical_date = last_date-datetime.timedelta(days=historical_days)\n",
        "    \n",
        "    dates_df=pd.DataFrame()\n",
        "    dates_df[\"date\"] = pd.date_range(start=historical_date, end=last_date)\n",
        "    dates_df[\"last_crisis_day\"] = np.nan\n",
        "    dates_df.loc[(dates_df['date'] == pd.Timestamp(2018, 12, 21)) | (dates_df['date'] == pd.Timestamp(2020, 3, 13)) | (dates_df['date'] == pd.Timestamp(2022, 4, 29)), 'last_crisis_day'] = dates_df['date']\n",
        "    dates_df.sort_values(by = 'date', axis = 0, ascending = True, inplace = True)\n",
        "    dates_df.ffill(axis = 0, inplace = True)\n",
        "    dates_df.sort_values(by = 'date', axis = 0, ascending = False, inplace = True)\n",
        "    dates_df[\"days_after_crisis\"] = dates_df[\"date\"] - dates_df[\"last_crisis_day\"] \n",
        "    dates_df.set_index([\"date\"], inplace = True)\n",
        "    dates_df['days_after_crisis'] = pd.to_numeric(dates_df['days_after_crisis'].dt.days, downcast='integer')\n",
        "    dates_df.drop([\"last_crisis_day\"], axis = 1, inplace = True)\n",
        "    return dates_df"
      ],
      "metadata": {
        "id": "gL7WcmKmO2jw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from datetime import date\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from selenium import webdriver\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support.select import Select\n",
        "import io\n",
        "  \n",
        "#symbol = \"AXP\"\n",
        "\n",
        "def search_symbol(symbol, driver):\n",
        "    elem = driver.find_element_by_id(\"ticker\")\n",
        "    elem.send_keys(symbol)\n",
        "    elem.send_keys(Keys.RETURN)\n",
        "    driver.implicitly_wait(3)\n",
        "\n",
        "def find_estim_tables(driver):\n",
        "    '''\n",
        "    get a table with earnings on a page  ZACK\n",
        "    '''\n",
        "    #Find required table\n",
        "    elem = driver.find_element_by_xpath('//*[@id=\"earnings_announcements_earnings_table\"]') \n",
        "    #get all rows from the table\n",
        "    rows = [row.text.encode(\"utf8\") for row in elem.find_elements_by_tag_name('tr')]\n",
        "    #convert list of bytes to list of strings\n",
        "    rows = [row.decode(\"utf-8\").replace(\"\\n\",\" \") for row in rows]\n",
        "    #convert list of strings to a dataframe \n",
        "    earnings_history_df = pd.read_csv(io.StringIO('\\n'.join(rows)), delim_whitespace=True, header = 0, names = ['date', 'Period_Ending', 'Estimate', 'Reported', 'Surprise', 'Surprise_%', \"str1\", \"str2\"])\n",
        "    \n",
        "    #Find the latest estimate \n",
        "    elem = driver.find_element_by_xpath('//*[@id=\"right_content\"]/section[2]/div') \n",
        "    #get all rows from the table\n",
        "    rows1 = [row.text.encode(\"utf8\") for row in elem.find_elements_by_tag_name('td')]\n",
        "    rows2 = [row.text.encode(\"utf8\") for row in elem.find_elements_by_tag_name('th')]\n",
        "    #convert list of bytes to list of strings\n",
        "    rows1 = [row.decode(\"utf-8\").replace(\"\\n\",\" \") for row in rows1]\n",
        "    rows2 = [row.decode(\"utf-8\").replace(\"\\n\",\" \") for row in rows2]    \n",
        "    #convert list of strings to a dataframe \n",
        "    earnings_latest_df = pd.DataFrame(rows1).transpose()\n",
        "    earnings_latest_df.columns =  ['Period_Ending', 'Estimate', 'Surprise_%']\n",
        "    earnings_latest_df[\"date\"] = rows2[-1].split(\" \")[0]\n",
        "    \n",
        "    earnings = pd.concat([earnings_latest_df, earnings_history_df], ignore_index = True, sort = False)\n",
        "    earnings['date'] = pd.to_datetime(earnings['date'])\n",
        "    earnings.set_index([\"date\"], inplace = True)\n",
        "    \n",
        "    return earnings\n",
        "\n",
        "def find_divid_tables(driver):\n",
        "    '''\n",
        "    get a table with dividends on a page  ZACK\n",
        "    '''\n",
        "    element = driver.find_element_by_xpath('//*[@id=\"earnings_announcements_tabs\"]/ul')\n",
        "    driver.execute_script('arguments[0].scrollIntoView({block: \"center\", inline: \"center\"})', element)\n",
        "    \n",
        "    elem = driver.find_element_by_xpath('//*[@id=\"ui-id-7\"]')\n",
        "    elem.click()\n",
        "    time.sleep(3)\n",
        "    #Expand 100 records\n",
        "    dropdown = driver.find_element_by_name(\"earnings_announcements_dividends_table_length\")\n",
        "    Select(dropdown).select_by_visible_text(\"100\")\n",
        "    \n",
        "    #Find required table\n",
        "    elem = driver.find_element_by_xpath('//*[@id=\"earnings_announcements_dividends_table\"]') \n",
        "    #get all rows from the table\n",
        "    rows = [row.text.encode(\"utf8\") for row in elem.find_elements_by_tag_name('tr')]\n",
        "    #convert list of bytes to list of strings\n",
        "    rows = [row.decode(\"utf-8\").replace(\"\\n\",\" \") for row in rows]\n",
        "    #convert list of strings to a dataframe \n",
        "    \n",
        "    dividends_history_df = pd.read_csv(io.StringIO('\\n'.join(rows)), delim_whitespace=True, header = 0, names = ['Date_Paid', 'Amount', 'Date_Announced', 'Ex-Dividend_Date'])\n",
        "    dividends_history_df.dropna(axis = 0, inplace = True)\n",
        "    dividends_history_df['Date_Paid'] = pd.to_datetime(dividends_history_df['Date_Paid'])\n",
        "    dividends_history_df['Date_Announced'] = pd.to_datetime(dividends_history_df['Date_Announced'])\n",
        "    dividends_history_df['Ex-Dividend_Date'] = pd.to_datetime(dividends_history_df['Ex-Dividend_Date'])\n",
        "    dividends_history_df.rename(columns = {\"Date_Announced\":\"date\"}, inplace = True)\n",
        "    dividends_history_df.set_index([\"date\"], inplace = True)\n",
        "    \n",
        "    return dividends_history_df\n",
        "\n",
        "def get_earn_and_dividends(symbol):\n",
        "    '''\n",
        "    This function launches browser for data load and fetches earnings and dividends data\n",
        "    '''\n",
        "    #Start Chrome \n",
        "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
        "    #Go to the website\n",
        "    driver.get('https://www.zacks.com/stock/research/CSCO/earnings-calendar')\n",
        "    \n",
        "    #Search stock\n",
        "    search_symbol(symbol, driver)\n",
        "    \n",
        "    #expand_100_earnings values:\n",
        "    dropdown = driver.find_element_by_name(\"earnings_announcements_earnings_table_length\")\n",
        "    Select(dropdown).select_by_visible_text(\"100\")\n",
        "    time.sleep(3)\n",
        "    \n",
        "    #Get earnings\n",
        "    earnings = find_estim_tables(driver)\n",
        "    time.sleep(3)\n",
        "    \n",
        "    #Ge dividends\n",
        "    dividends = find_divid_tables(driver)\n",
        "    \n",
        "    #Close browser\n",
        "    driver.close()\n",
        "    \n",
        "    #Transforming Earnings dataframe to a final version\n",
        "    #Transform string values to numeric\n",
        "    earnings.replace({\"--\":np.nan},inplace = True)\n",
        "    earnings.Reported, earnings.Estimate, earnings[\"Surprise_%\"] = pd.to_numeric(earnings.Reported.str.replace(\"$\",\"\")), pd.to_numeric(earnings.Estimate.str.replace(\"$\",\"\")), pd.to_numeric(earnings[\"Surprise_%\"].str.replace(\"%\",\"\").str.replace(\",\",\"\"))\n",
        "    earnings[\"surprise_%\"] = earnings[\"Surprise_%\"]/100\n",
        "    earnings[\"date_of_report\"] = earnings.index\n",
        "    #getting expected future earnings change\n",
        "    earnings[\"future_estimate\"] = earnings.Estimate.shift(1)\n",
        "    earnings[\"previous_surprise\"] = earnings[\"surprise_%\"].shift(-1)\n",
        "    earnings[\"expected_growth\"]= (earnings.future_estimate - earnings.Reported)/earnings.Reported\n",
        "    earnings = earnings[[\"surprise_%\", \"expected_growth\", \"previous_surprise\", \"date_of_report\"]]\n",
        "    \n",
        "    #Transforming Dividends dataframe to a final version\n",
        "    #STR to value\n",
        "    dividends.replace({\"--\":np.nan},inplace = True)\n",
        "    dividends.Amount = pd.to_numeric(dividends.Amount.str.replace(\"$\",\"\"))\n",
        "    #get date that we later can use to count days after the announcement\n",
        "    dividends[\"date_announced\"] = dividends.index\n",
        "    #Getting dividends trend\n",
        "    dividends[\"previous_divid\"] = dividends.Amount.shift(-1)\n",
        "    dividends[\"dividends_change\"] = (dividends.Amount - dividends.previous_divid)/dividends.previous_divid\n",
        "    dividends = dividends[dividends.dividends_change != 0]\n",
        "    dividends[\"prev_div_change\"] = dividends.dividends_change.shift(-1)\n",
        "    dividends = dividends[[\"dividends_change\",\"prev_div_change\",\"date_announced\"]]\n",
        "    \n",
        "    #earnings = a.copy()\n",
        "    #dividends = b.copy()\n",
        "    \n",
        "    #Match earnings with dates\n",
        "    #Creating Dates dataframe with all possible dates values\n",
        "    dates_df=pd.DataFrame()\n",
        "    dates_df[\"date\"] = pd.date_range(start=earnings.index.min(), end=earnings.index.max())\n",
        "    #Set dates column as index\n",
        "    dates_df.set_index([\"date\"], inplace = True)\n",
        "    #Creating a dates_earnings dataset where we extrapolate existing quarterly data to daily\n",
        "    dates_earnings = dates_df.copy()\n",
        "    dates_earnings = dates_earnings.join(earnings, how = 'left')\n",
        "    dates_earnings.sort_values(by = 'date', axis = 0, ascending = True, inplace = True)\n",
        "    dates_earnings.ffill(axis = 0, inplace = True)\n",
        "    dates_earnings.sort_values(by = 'date', axis = 0, ascending = False, inplace = True)\n",
        "    dates_earnings[\"days_after_earn_report\"] = dates_earnings.index - dates_earnings[\"date_of_report\"] \n",
        "    dates_earnings['days_after_earn_report'] = pd.to_numeric(dates_earnings['days_after_earn_report'].dt.days, downcast='integer')\n",
        "    dates_earnings.drop([\"date_of_report\"], axis = 1, inplace = True)\n",
        "    \n",
        "    #Match dividends with dates\n",
        "    #Creating Dates dataframe with all possible dates values\n",
        "    if dividends.empty:\n",
        "        dates_dividends = pd.DataFrame(columns = [\"days_after_divid_report\", \"dividends_change\",\"prev_div_change\"])\n",
        "        dates_dividends.index.names = ['date']\n",
        "    else:\n",
        "        dates_df=pd.DataFrame()\n",
        "        dates_df[\"date\"] = pd.date_range(start=dividends.index.min(), end=date.today())\n",
        "        #Set dates column as index\n",
        "        dates_df.set_index([\"date\"], inplace = True)\n",
        "        #Creating a dates_ividends dataset where we extrapolate existing quarterly data to daily\n",
        "        dates_dividends = dates_df.copy()\n",
        "        dates_dividends = dates_dividends.join(dividends, how = 'left')\n",
        "        dates_dividends.sort_values(by = 'date', axis = 0, ascending = True, inplace = True)\n",
        "        dates_dividends.ffill(axis = 0, inplace = True)\n",
        "        dates_dividends.sort_values(by = 'date', axis = 0, ascending = False, inplace = True)\n",
        "        dates_dividends[\"days_after_divid_report\"] = dates_dividends.index - dates_dividends[\"date_announced\"] \n",
        "        dates_dividends['days_after_divid_report'] = pd.to_numeric(dates_dividends['days_after_divid_report'].dt.days, downcast='integer')\n",
        "        dates_dividends.drop([\"date_announced\"], axis = 1, inplace = True)\n",
        "    \n",
        "    return dates_earnings, dates_dividends"
      ],
      "metadata": {
        "id": "dFp1_t3uO4ED"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import date\n",
        "import VIX\n",
        "import new_earnings\n",
        "import tiingo_data as tii\n",
        "import US_bond\n",
        "import crisis_dataset\n",
        "\n",
        "script_location = os.getcwd()\n",
        "\n",
        "#get VIX Volatility index\n",
        "vix_df = VIX.get_vix()\n",
        "\n",
        "#get 10Y_bond index\n",
        "usa_bond = US_bond.download_tables()\n",
        "\n",
        "#get last crisis date data\n",
        "last_crisis = crisis_dataset.get_dates()\n",
        "\n",
        "#Get all companies sector, industry, location, etc\n",
        "metadata = tii.fetch_metadata()\n",
        "\n",
        "# Get earnings esteem\n",
        "dow30  = [\"AXP\", \"AMGN\", \"AAPL\", \"BA\", \"CAT\", \"CSCO\", \"CVX\", \"GS\", \"HD\", \"HON\", \"IBM\", \"INTC\", \"JNJ\", \"KO\", \"JPM\", \"MCD\", \"MMM\", \"MRK\", \"MSFT\", \"NKE\", \"PG\", \"TRV\", \"UNH\", \"CRM\", \"VZ\", \"V\", \"WBA\", \"WMT\", \"DIS\", \"DOW\"]\n",
        "\n",
        "todays_date = date.today()\n",
        "\n",
        "#a stock symbol to get data\n",
        "for symbol in dow30[:]:\n",
        "    #How long historical data we need\n",
        "    historical_dates_range = 1450\n",
        "    #How long Moving Average we take\n",
        "    MA_gener_days = 180\n",
        "    \n",
        "    # Hitorical Prices Open, Close, Low, High, Volume for historical data\n",
        "    stock_dataset = tii.fetch_stock(symbol, todays_date, historical_dates_range)\n",
        "    # Adding a Moving Average and price position over/below MA to the previous dataset\n",
        "    #data = tii.create_dataset(stock_dataset, MA_gener_days)\n",
        "    # Getting stock market cap, PE Ratio, PB Ratio\n",
        "    fundamentals  =   tii.fetch_fundamentals(symbol, todays_date, historical_dates_range)\n",
        "    # Financial KPIs\n",
        "    statements =   tii.fetch_statements(symbol, todays_date, historical_dates_range)\n",
        "    # Get stock sector and industry\n",
        "    stock_metadata = metadata[metadata.ticker == symbol.lower()][[\"sector\",\"industry\"]].copy()\n",
        "    # stock dividents & earnings\n",
        "    stock_earnings, stock_dividends = new_earnings.get_earn_and_dividends(symbol)    \n",
        "    \n",
        "    # Combining all stock's data\n",
        "    big_dataset = tii.combine_tables(stock_dataset, statements, fundamentals, stock_metadata, historical_dates_range)\n",
        "    big_dataset = big_dataset.join(vix, how = 'left')\n",
        "    big_dataset.dropna(inplace = True)\n",
        "    \n",
        "    #Combining data with US Bonds\n",
        "    big_dataset = big_dataset.join(usa_bond, how = 'left')\n",
        "    #Combining data with earnings & dividends info\n",
        "    big_dataset = big_dataset.join(stock_earnings, how = 'left')\n",
        "    big_dataset = big_dataset.join(stock_dividends, how = 'left')\n",
        "    \n",
        "    big_dataset.sort_values(by = 'date', axis = 0, ascending = False, inplace = True)\n",
        "    big_dataset[\"10Y_bond_MoM\"] = big_dataset[\"10Y_bonds\"]-big_dataset[\"10Y_bonds\"].shift(-22)\n",
        "    big_dataset[\"Debt-to-Equity_Ratio\"] = big_dataset[\"totalAssets\"]/big_dataset[\"totalLiabilities\"]\n",
        "    big_dataset[\"DividendsYield\"] = big_dataset[\"payDiv\"]/big_dataset[\"marketCap\"]\n",
        "    big_dataset[\"PayoutRatio\"] = big_dataset[\"payDiv\"]/big_dataset[\"grossProfit\"]\n",
        "    big_dataset[\"Acc_Rec_Pay_Ration\"] = big_dataset[\"acctRec\"]/big_dataset[\"acctPay\"]\n",
        "    big_dataset[\"Earnings_per_stock\"] = big_dataset[\"epsDil\"]/big_dataset[\"close\"]\n",
        "    big_dataset[\"future_30dprice_change\"] = (big_dataset[\"close\"].shift(22) - big_dataset[\"close\"])/big_dataset[\"close\"].shift(22)\n",
        "    #big_dataset = big_dataset[[\"roe\", \"longTermDebtEquity\", \"grossMargin\", \"revenueQoQ\", \"rps\", \"epsQoQ\", \"piotroskiFScore\", \"currentRatio\", \"roa\", \"profitMargin\",\"peRatio\", \"pbRatio\",\"trailingPEG1Y\",\"VIX_high\",\"sector\",\"industry\",\"10Y_bonds\", \"10Y_bond_MoM\",\"Debt-to-Equity_Ratio\",\"DividendsYield\",\"PayoutRatio\",\"Acc_Rec_Pay_Ration\",\"Earnings_per_stock\",\"dividends_change\",\"prev_div_change\",\"days_after_divid_report\",\"surprise_%\", \"expected_growth\", \"previous_surprise\",\"days_after_earn_report\"]]\n",
        "    \n",
        "    #TO DO \n",
        "    #CALC Debt-to-Equity_Ratio,\tDividendsYield,\tPayout ratio, Acc_Rec_Pay_Ration, YoY Debt ratio, YoY eps, YoY investments\n",
        "    #REPLACE DividendsYield,\tPayout ratio by estimates in Future Periods\n",
        "    \n",
        "    csv_file = symbol + \".csv\"\n",
        "    path_to_all_data = os. path. join(script_location, \"CSVs\")\n",
        "    path_to_all_data = os. path. join(path_to_all_data, csv_file)\n",
        "    big_dataset.to_csv(path_to_all_data)"
      ],
      "metadata": {
        "id": "KlsIy8YfO-xR"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}
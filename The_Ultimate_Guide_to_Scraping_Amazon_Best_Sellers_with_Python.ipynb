{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6VvBIzBrvHofx2Udrlkez"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@datajournal/scraping-amazon-best-sellers-2781b1399bc9)"
      ],
      "metadata": {
        "id": "oNND9ZYMP0K0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install Python"
      ],
      "metadata": {
        "id": "-1w_MXGzQC9K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsOJrqoQPsSh",
        "outputId": "71c4d9ad-85fe-414b-eff7-db3c66252b07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.27.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting webdriver-manager\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.28.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.12.14)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from webdriver-manager) (2.32.3)\n",
            "Collecting python-dotenv (from webdriver-manager)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from webdriver-manager) (24.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.3.0)\n",
            "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver-manager) (3.4.0)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Downloading selenium-4.27.1-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading trio-0.28.0-py3-none-any.whl (486 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.3/486.3 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Installing collected packages: sortedcontainers, wsproto, python-dotenv, outcome, webdriver-manager, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 python-dotenv-1.0.1 selenium-4.27.1 sortedcontainers-2.4.0 trio-0.28.0 trio-websocket-0.11.1 webdriver-manager-4.0.2 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install selenium webdriver-manager pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Inspecting Amazon’s Best Sellers Page\n",
        "- Product title: The product title is located within a specific HTML element with a class that uniquely identifies it.\n",
        "Sellers Page\n",
        "- Product price: The price is located within another specific HTML element.\n",
        "Sellers Page\n",
        "- Product URL: Each product has a URL linking to its detailed product page."
      ],
      "metadata": {
        "id": "b_x6iwk1QFuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Setting Up Selenium for Web Scraping"
      ],
      "metadata": {
        "id": "81lk4B4rQNTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "import pandas as pd\n",
        "import time"
      ],
      "metadata": {
        "id": "4uKVpdqYQElm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_chrome_driver():\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\" - headless\")\n",
        "    service = Service(ChromeDriverManager().install())\n",
        "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "    return driver"
      ],
      "metadata": {
        "id": "aXvabopqQO5T"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Writing the Scraping Logic"
      ],
      "metadata": {
        "id": "HDQeP2ozQSQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_products_from_page(url, driver):\n",
        "    driver.get(url)\n",
        "    time.sleep(3) # Wait for the page to load\n",
        "    # Find all products on the page\n",
        "    product_elements = driver.find_elements(By.CLASS_NAME, \"zg-item\")\n",
        "    # List to store product data\n",
        "    products = []\n",
        "    # Loop through the products and extract data\n",
        "    for product in product_elements:\n",
        "        try:\n",
        "            title = product.find_element(By.CLASS_NAME, \"p13n-sc-truncate\").text\n",
        "            url = product.find_element(By.CLASS_NAME, \"a-link-normal\").get_attribute(\"href\")\n",
        "            price = product.find_element(By.CLASS_NAME, \"p13n-sc-price\").text\n",
        "            products.append({\"title\": title, \"url\": url, \"price\": price})\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting product data: {e}\")\n",
        "            continue\n",
        "    return products"
      ],
      "metadata": {
        "id": "8Y1VtVxrQRSy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Exporting Data to CSV"
      ],
      "metadata": {
        "id": "xwZOIG4EQbe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_csv(products, filename):\n",
        "    df = pd.DataFrame(products)\n",
        "    df.to_csv(filename, index=False)"
      ],
      "metadata": {
        "id": "S2f7eJeZQaTm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Putting It All Together"
      ],
      "metadata": {
        "id": "tv1bLvXHQiEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    url = \"https://www.amazon.com/Best-Sellers-Kitchen-Dining/zgbs/kitchen/\"\n",
        "    driver = init_chrome_driver()\n",
        "    try:\n",
        "        products = get_products_from_page(url, driver)\n",
        "        save_to_csv(products, \"amazon_best_sellers.csv\")\n",
        "    finally:\n",
        "        driver.quit()\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "wI_NgfZLQg5r"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Running the Script\n",
        "```\n",
        "python main.py\n",
        "```"
      ],
      "metadata": {
        "id": "Asj0OBD7QnPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Handling Scraping Challenges\n",
        "\n",
        "1. Rate-limiting: Amazon can block your IP address if it detects too many requests in a short period. To avoid this, implement a delay between requests using time.sleep().\n",
        "2. CAPTCHA: Amazon uses CAPTCHAs to prevent bots from scraping their site. Selenium cannot solve CAPTCHAs, so you may need a service like 2Captcha to bypass them.\n",
        "3. IP blocking: To prevent your IP from being blocked, consider using a proxy service like ScraperAPI or rotating IP addresses."
      ],
      "metadata": {
        "id": "ajcGi4o1Qvx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9: Scraping More Categories"
      ],
      "metadata": {
        "id": "1CH8X_qCQ1kR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.amazon.com/Best-Sellers-Books/zgbs/books/\""
      ],
      "metadata": {
        "id": "9rqgMEWyQskR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 10: Using an Amazon Scraping API (Alternative Method)"
      ],
      "metadata": {
        "id": "UdgfJ02sQ-EG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "def scrape_amazon_api():\n",
        "    payload = {\n",
        "    \"source\": \"amazon_bestsellers\",\n",
        "    \"domain\": \"com\",\n",
        "    \"query\": \"284507\",\n",
        "    \"render\": \"html\",\n",
        "    \"start_page\": 1,\n",
        "    \"parse\": True,\n",
        "    }\n",
        "    response = requests.post(\"https://realtime.oxylabs.io/v1/queries\", json=payload, auth=(\"USERNAME\", \"PASSWORD\"))\n",
        "    data = response.json()\n",
        "    # Process the data and save it to CSV\n",
        "    products = data[\"results\"][0][\"content\"][\"results\"]\n",
        "    df = pd.DataFrame(products)\n",
        "    df.to_csv(\"amazon_products_api.csv\", index=False)\n",
        "scrape_amazon_api()"
      ],
      "metadata": {
        "id": "SqLra4v0Q8Ym"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}
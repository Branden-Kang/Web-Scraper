{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUpbW9HQV7oTqLMjD40wXU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@ccpythonprogramming/web-scraping-with-python-an-example-using-sqlite-c23912e96336)"
      ],
      "metadata": {
        "id": "oSiVqP-gmN-C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6mLuLB7xmDHa"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import sqlite3\n",
        "import time\n",
        "\n",
        "# URL of the job listings page\n",
        "URL = \"https://example.com/jobs/data-scientist\"\n",
        "\n",
        "# SQLite database file\n",
        "DB_FILE = \"jobs.db\"\n",
        "\n",
        "def create_database():\n",
        "    conn = sqlite3.connect(DB_FILE)\n",
        "    c = conn.cursor()\n",
        "    c.execute('''CREATE TABLE IF NOT EXISTS job_listings\n",
        "                 (id INTEGER PRIMARY KEY, title TEXT, company TEXT, location TEXT, date_posted TEXT, url TEXT)''')\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def scrape_jobs():\n",
        "    response = requests.get(URL)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    jobs = []\n",
        "    listings = soup.find_all('div', class_='job-listing')\n",
        "\n",
        "    for listing in listings:\n",
        "        title = listing.find('h2', class_='job-title').text.strip()\n",
        "        company = listing.find('div', class_='company').text.strip()\n",
        "        location = listing.find('div', class_='location').text.strip()\n",
        "        date_posted = listing.find('div', class_='date-posted').text.strip()\n",
        "        job_url = listing.find('a', class_='apply-link')['href']\n",
        "\n",
        "        jobs.append((title, company, location, date_posted, job_url))\n",
        "\n",
        "    return jobs\n",
        "\n",
        "def save_jobs_to_db(jobs):\n",
        "    conn = sqlite3.connect(DB_FILE)\n",
        "    c = conn.cursor()\n",
        "    c.executemany('INSERT INTO job_listings (title, company, location, date_posted, url) VALUES (?, ?, ?, ?, ?)', jobs)\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_database()\n",
        "    while True:\n",
        "        jobs = scrape_jobs()\n",
        "        save_jobs_to_db(jobs)\n",
        "        print(f\"{len(jobs)} jobs scraped and saved to database.\")\n",
        "        time.sleep(3600)  # Wait for 1 hour before scraping again"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOatO83+36VgtVcGVFMnzhZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://python.plainenglish.io/mastering-web-scraping-pipelines-with-python-e2db8dc15d4f)"
      ],
      "metadata": {
        "id": "293vuF_PjMDZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYad9-bcjJNg",
        "outputId": "b5cb29c4-4052-4e9d-f954-9e8fe0818ab1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (4.11.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (2.27.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4) (2.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests) (3.4)\n"
          ]
        }
      ],
      "source": [
        "pip install beautifulsoup4 requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import concurrent.futures\n",
        "\n",
        "def scrape_website_1():\n",
        "    url = \"https://www.example1.com\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    # Your scraping logic here\n",
        "    return data\n",
        "\n",
        "def scrape_website_2():\n",
        "    url = \"https://www.example2.com\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    # Your scraping logic here\n",
        "    return data"
      ],
      "metadata": {
        "id": "31HLZFjnjOYi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_scrapers():\n",
        "    scrapers = [scrape_website_1, scrape_website_2]  # Add more scrapers if needed\n",
        "    data = []\n",
        "\n",
        "    for scraper in scrapers:\n",
        "        try:\n",
        "            result = scraper()\n",
        "            data.append(result)\n",
        "        except Exception as e:\n",
        "            print(f\"Error while running scraper {scraper.__name__}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "T-kqYcPxjQKN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    scraped_data = run_scrapers()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcBkSJ8ZjROf",
        "outputId": "c52b811b-db52-4d09-e645-6175ebee1ba5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error while running scraper scrape_website_1: name 'data' is not defined\n",
            "Error while running scraper scrape_website_2: HTTPSConnectionPool(host='www.example2.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1129)')))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_scrapers_parallel():\n",
        "    scrapers = [scrape_website_1, scrape_website_2]  # Add more scrapers if needed\n",
        "    data = []\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=len(scrapers)) as executor:\n",
        "        future_to_scraper = {executor.submit(scraper): scraper for scraper in scrapers}\n",
        "\n",
        "        for future in concurrent.futures.as_completed(future_to_scraper):\n",
        "            scraper = future_to_scraper[future]\n",
        "\n",
        "            try:\n",
        "                result = future.result()\n",
        "                data.append(result)\n",
        "            except Exception as e:\n",
        "                print(f\"Error while running scraper {scraper.__name__}: {e}\")\n",
        "                continue\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "G4hM0Lm4jU-a"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    scraped_data_parallel = run_scrapers_parallel()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nC9F9MLNjV6K",
        "outputId": "c797645b-2031-4940-9f61-b36ad3c772f2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error while running scraper scrape_website_2: HTTPSConnectionPool(host='www.example2.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1129)')))\n",
            "Error while running scraper scrape_website_1: name 'data' is not defined\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "How I scrape lots of sites with one python script.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO9zyW761P9/BFVRkWQ7Kzz"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://levelup.gitconnected.com/how-i-scrape-lots-of-sites-with-one-python-script-9fba09d5c9be)"
      ],
      "metadata": {
        "id": "Wn9ukdhEfjwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeASc4ByfrS-",
        "outputId": "fb0b6956-2812-41e5-c524-e946c822b414"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.1.2-py3-none-any.whl (963 kB)\n",
            "\u001b[K     |████████████████████████████████| 963 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting trio~=0.17\n",
            "  Downloading trio-0.20.0-py3-none-any.whl (359 kB)\n",
            "\u001b[K     |████████████████████████████████| 359 kB 59.5 MB/s \n",
            "\u001b[?25hCollecting urllib3[secure,socks]~=1.26\n",
            "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 62.4 MB/s \n",
            "\u001b[?25hCollecting outcome\n",
            "  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.4.0)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
            "Collecting cryptography>=1.3.4\n",
            "  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 43.6 MB/s \n",
            "\u001b[?25hCollecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (3.10.0.2)\n",
            "Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.8 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-36.0.1 h11-0.13.0 outcome-1.1.0 pyOpenSSL-22.0.0 selenium-4.1.2 sniffio-1.2.0 trio-0.20.0 trio-websocket-0.9.2 urllib3-1.26.8 wsproto-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TuzDmhg2fcYe"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver import Chrome, ChromeOptions\n",
        "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
        "\n",
        "def get_local_safe_setup():\n",
        "    options = ChromeOptions() \n",
        "    options.add_argument(\"--disable-blink-features\")\n",
        "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "    options.add_argument(\"--disable-infobars\")\n",
        "    options.add_argument(\"--disable-popup-blocking\")\n",
        "    options.add_argument(\"--disable-notifications\")\n",
        "\n",
        "    driver = Chrome(desired_capabilities = options.to_capabilities())\n",
        "\n",
        "    return driver\n",
        "\n",
        "def get_safe_setup():\n",
        "    options = ChromeOptions() \n",
        "    options.add_argument(\"--disable-dev-shm-usage\") \n",
        "    options.add_argument(\"--disable-blink-features\")\n",
        "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "    options.add_argument(\"--disable-infobars\")\n",
        "    options.add_argument(\"--disable-popup-blocking\")\n",
        "    options.add_argument(\"--disable-notifications\")\n",
        "\n",
        "    driver = webdriver.Remote(\"http://127.0.0.1:4444/wd/hub\", desired_capabilities = options.to_capabilities())\n",
        "\n",
        "    return driver\n",
        "\n",
        "def get_text_by_selector(container, selector):\n",
        "    elem = container.find_elements_by_class_name(selector)\n",
        "\n",
        "    if len(elem) > 0:\n",
        "        return next(iter(elem)).text.replace('\\n',' ').strip()\n",
        "    else: \n",
        "        print(f'Missing value for selector {selector}')\n",
        "        return ''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import argparse\n",
        "\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "class Spider:\n",
        "    def __init__(self, driver, config):\n",
        "        self.__driver = driver\n",
        "        self.__config = config\n",
        "    \n",
        "    def parse(self, url: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "            Scrapes a website from url using predefined config, returns DataFrame\n",
        "            parameters:\n",
        "                url: string\n",
        "            \n",
        "            returns:\n",
        "                pandas Dataframe\n",
        "        \"\"\"\n",
        "        self.__driver.get(url)\n",
        "  \n",
        "        container_element = WebDriverWait(self.__driver, 5).until(\n",
        "            EC.presence_of_element_located((By.CLASS_NAME, self.__config['container_class']))\n",
        "        )\n",
        "            \n",
        "        items = self.__driver.find_elements_by_class_name(self.__config['items_class'])\n",
        "        items_content = [\n",
        "            [get_text_by_selector(div, selector) for selector in self.__config['data_selectors']]\n",
        "            for div in items]\n",
        "        return pd.DataFrame(items_content, columns = self.__config['data_column_titles']) \n",
        "\n",
        "    def parse_pages(self, url: str):\n",
        "        \"\"\"\n",
        "            Scrapes a website with pagination from url using predefined config, yields list of pandas DataFrames\n",
        "            parameters:\n",
        "                url: string\n",
        "        \"\"\"\n",
        "        pagination_config = self.__config['pagination']        \n",
        "        \n",
        "        for i in tqdm(range(1, pagination_config['crawl_pages'] + 1)):\n",
        "            yield self.parse(url.replace(\"$p$\", str(i)))\n",
        "\n",
        "            time.sleep(int(pagination_config['delay']/1000))      \n",
        "\n",
        "def scrape(args): \n",
        "    config = load_config(args.config)\n",
        "\n",
        "    pagination_config = config['pagination']\n",
        "    url = config['url']\n",
        "\n",
        "    driver = get_safe_setup()\n",
        "\n",
        "    spider = Spider(driver, config)\n",
        "\n",
        "    os.makedirs(os.path.dirname(args.output), exist_ok = True)\n",
        "\n",
        "    try:\n",
        "        if pagination_config['crawl_pages'] > 0:\n",
        "            data = spider.parse_pages(url)\n",
        "            df = pd.concat(list(data), axis = 0)\n",
        "        else:\n",
        "            df = spider.parse(url)\n",
        "        \n",
        "        df.to_csv(args.output, index = False)\n",
        "    except Exception as e:\n",
        "        print(f'Parsing failed due to {str(e)}')\n",
        "    finally:\n",
        "        driver.quit()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-c', '--config', help='Configuration of spider learning')\n",
        "    parser.add_argument('-o', '--output', help='Output file path')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    scrape(args)"
      ],
      "metadata": {
        "id": "x0ANt03UfnhH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # quotes.yaml\n",
        "# url: https://quotes.toscrape.com/page/$p$/\n",
        "# container_class: col-md-8\n",
        "# items_class: quote\n",
        "# data_selectors:\n",
        "#   - text\n",
        "#   - author\n",
        "#   - keywords\n",
        "# data_column_titles:\n",
        "#   - Text\n",
        "#   - Author\n",
        "#   - Keywords\n",
        "# pagination:\n",
        "#  crawl_pages: 5\n",
        "#  delay: 5000"
      ],
      "metadata": {
        "id": "Iz_r5Z59fsfH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# python -m spider -c \"./configs/quotes.yaml\" -o \"./outputs/quotes/$(date +%Y-%m-%d).csv\""
      ],
      "metadata": {
        "id": "a0f1MOkJf8Zs"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM97gIgqGX5MpLf593zgRFB"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@abdelilah.moulida/mastering-web-crawling-exploring-website-urls-with-python-d0dd9a35a7bc)"
      ],
      "metadata": {
        "id": "if6OLtbiG5Ys"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "48QIadEeG3Vg"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://www.example.com'\n",
        "response = requests.get(url)\n",
        "html_content = response.text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Assuming we have the HTML content stored in the `html_content` variable\n",
        "soup = BeautifulSoup(html_content, 'html.parser')"
      ],
      "metadata": {
        "id": "-31Ps2eZG8oh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anchors = soup.find_all('a')\n",
        "\n",
        "for anchor in anchors:\n",
        "    print(anchor['href'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6w8vENeG-WX",
        "outputId": "23818f85-b288-49cc-d444-06eb4899d0ed"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.iana.org/domains/example\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "visited_urls = set()\n",
        "\n",
        "def crawl(url):\n",
        "    if url in visited_urls:\n",
        "        return\n",
        "    visited_urls.add(url)\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        html_content = response.text\n",
        "\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        # Extracting URLs from anchor tags\n",
        "        anchors = soup.find_all('a')\n",
        "        for anchor in anchors:\n",
        "            href = anchor['href']\n",
        "            if href.startswith('http'):\n",
        "                crawl(href)\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print('An error occurred:', e)\n",
        "\n",
        "# Start crawling from a seed URL\n",
        "seed_url = 'https://www.example.com'\n",
        "crawl(seed_url)"
      ],
      "metadata": {
        "id": "VPdtNaS3G_dS"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "How to Create a Stock News Web Crawler with Python.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM3X+LARYtRX0nFgjDlMH9X"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://python.plainenglish.io/coding-methodology-stock-news-web-crawler-with-python-the-star-19c7586d7fd0)"
      ],
      "metadata": {
        "id": "WN2rW3c403tG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nPkL07t0xoL",
        "outputId": "b9828ab0-6cbd-420f-99b9-da43a6396694"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import traceback\n",
        "\n",
        "def item_search(item, limit, page):\n",
        "    stocknews = f\"https://www.thestar.com.my/search/?q={item}&qsort=oldest&qrec={limit}&qstockcode=&pgno={page}\"\n",
        "\n",
        "    html = requests.get(stocknews).text\n",
        "\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    return soup\n",
        "\n",
        "def get_details(url):\n",
        "    html = requests.get(url).text\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    content = soup.find('div', {'id':'story-body'})\n",
        "    if content:\n",
        "        content.get_text(strip=True)\n",
        "        return content\n",
        "    else:\n",
        "        return \"\"\n",
        "def star_new_crawler(page, search_query, limit):\n",
        "\n",
        "    title = []\n",
        "    links = []\n",
        "    premium = []\n",
        "    new_type = []\n",
        "    contents = []\n",
        "    publishedDate = []\n",
        "    while True:\n",
        "        print(page)\n",
        "        try:\n",
        "            result = item_search(search_query, limit, page)\n",
        "            title += [x.get_text(strip=True) for x in result.find_all(\"h2\", {\"class\": \"f18\"})]\n",
        "            links += [x.find('a', {\"data-content-type\": \"Article\"})['href'] for x in\n",
        "                     result.find_all(\"h2\", {\"class\": \"f18\"})]\n",
        "            premium += [x.get_text(strip=True) for x in result.find_all(\"span\", {\"class\": \"biz-icon\"})]\n",
        "            new_type += [x.get_text(strip=True) for x in result.find_all(\"a\", {\"class\": \"kicker\"})]\n",
        "            contents += [get_details(x) for x in links]\n",
        "            publishedDate += [x.get_text(strip=True) for x in result.find_all(\"span\", {\"class\": \"timestamp\"})]\n",
        "         \n",
        "            if len(title) == 0:\n",
        "                break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            traceback.print_exc()\n",
        "    \n",
        "        page += 1\n",
        "\n",
        "    pd.DataFrame({'new_type': new_type, 'title': title, 'premium': premium, 'links': links, 'published_data': publishedDate,\n",
        "                 'contents': contents}).to_excel(f'{search_query}.xlsx', index=False)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    page = 1\n",
        "    search_query = 'AAPL'\n",
        "    limit = 30\n",
        "    star_new_crawler(page, search_query, limit)"
      ]
    }
  ]
}